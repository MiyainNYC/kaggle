#https://www.kaggle.com/c/prudential-life-insurance-assessment/forums/t/19016/3rd-place-solution-summary/108608

First off, luck was a big factor in this competition so let me try and alleviate some guilt:

Qingchen, I'm sorry for edging you out of 3rd place so narrowly. From your post, clearly you put serious effort into building out your ensemble. I know how frustrating it is to wish you'd just built one more tree into a model. Hang in there. Next time the dice could roll in your favor.

As for my solution, there are two logical parts. The first part computes class probabilities for each of the eight response levels. The second part maximizes the expected value of the Quadratic Weighted Kappa (QWK) for the given set of probabilities.

Class Probabilities: These were computed through stacking, so let me describe from the bottom up.

attributes

base attributes, number of keywords, a few other things suggested on the forum
2D tnse embedding
the 2D embedding generated by a 4096-256-16-2-16-256-4096 autoencoder
The first 30-dimensions of a SVD decomposition of the categorical features
kmeans clustering with 64 centers
quadratic interactions selected by lasso mse regression
nodes of a 256-tree 50-node random forest selected by lasso mse regression
level 1 models

tree based models: 8 xgboost models minimizing: mse, possion, multinomial, mae*, tukey*, or QWK* loss. A random forest thrown in for good measure.
knn: 8 k-nearest neighbor models with k from 50-1000
neural nets: 6 neural networks minimizing: mse, mae, multinomial, or QWK* loss
linear: 1 lasso mse regression
level 2 models

both use the level 1 models as inputs
multinomial xgb
multinomial neural net
QWK optimization: Uses the average of the level 2 models as the class probabilities.

randomly select a category 1-8 for every test example
iterate through the test examples one by one and change each class to whatever minimizes the expected value of the QWK based on the probabilites of the level 2 models.
repeat step 2. over the entire test set until convergence is reached
submit result
*Note: Some of the loss functions are custom implementations. For xgboost these are implemented in C++ and for the neural nets they are implemented in either Keras or Lasagne.

idle_speculation's image  idle_speculation  
========================CV strategy for 3rd place, 6 submissions and 3rd places=======
I'm sorry to disappoint, but my cross-validation strategy is quite ordinary. I broke the training set into 17 folds, trained on 16 and used the out-of-sample prediction on the remaining fold. Perhaps my conviction that the public leaderboard was unreliable made the difference.

Some monte-carlo simulations of the variance in the demominator of Quadratic Weighted Kappa were what convinced me. I tested sampling at size similar public leaderboard and found the standard deviation in the results was around 1%. That may sound small, but it's the difference between 1st place and 139th place in the final standings.

Since there were obvious differences between the train and test set, I can't really fault anyone for trying to fit the leaderboard. I know that I spent quite a bit of time convincing myself that the training set provided no evidence that the distribution of response in the test set was any different.

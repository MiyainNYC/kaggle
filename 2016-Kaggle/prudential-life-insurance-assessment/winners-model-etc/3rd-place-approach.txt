#https://www.kaggle.com/c/prudential-life-insurance-assessment/forums/t/19016/3rd-place-solution-summary/108608

First off, luck was a big factor in this competition so let me try and alleviate some guilt:

Qingchen, I'm sorry for edging you out of 3rd place so narrowly. From your post, clearly you put serious effort into building out your ensemble. I know how frustrating it is to wish you'd just built one more tree into a model. Hang in there. Next time the dice could roll in your favor.

As for my solution, there are two logical parts. The first part computes class probabilities for each of the eight response levels. The second part maximizes the expected value of the Quadratic Weighted Kappa (QWK) for the given set of probabilities.

Class Probabilities: These were computed through stacking, so let me describe from the bottom up.

attributes

base attributes, number of keywords, a few other things suggested on the forum
2D tnse embedding
the 2D embedding generated by a 4096-256-16-2-16-256-4096 autoencoder
The first 30-dimensions of a SVD decomposition of the categorical features
kmeans clustering with 64 centers
quadratic interactions selected by lasso mse regression
nodes of a 256-tree 50-node random forest selected by lasso mse regression
level 1 models

tree based models: 8 xgboost models minimizing: mse, possion, multinomial, mae*, tukey*, or QWK* loss. A random forest thrown in for good measure.
knn: 8 k-nearest neighbor models with k from 50-1000
neural nets: 6 neural networks minimizing: mse, mae, multinomial, or QWK* loss
linear: 1 lasso mse regression
level 2 models

both use the level 1 models as inputs
multinomial xgb
multinomial neural net
QWK optimization: Uses the average of the level 2 models as the class probabilities.

randomly select a category 1-8 for every test example
iterate through the test examples one by one and change each class to whatever minimizes the expected value of the QWK based on the probabilites of the level 2 models.
repeat step 2. over the entire test set until convergence is reached
submit result
*Note: Some of the loss functions are custom implementations. For xgboost these are implemented in C++ and for the neural nets they are implemented in either Keras or Lasagne.

idle_speculation's image  idle_speculation  

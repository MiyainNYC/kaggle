#https://www.kaggle.com/c/prudential-life-insurance-assessment/forums/t/19010/1st-place-solution

Hello all, here's my writeup. Hope you find it insightful (I certainly did learn a lot in the course of the competition)!

Feature engineering:

create dummy vars for Product_Info_2 (keep everything else as numeric)
calculate sum of all Medical_Keyword columns
for each binary keyword-value pair, calculate the mean of the target variable, then for each observation take the mean and the minimum of the keyword-value-meantargets
Modeling:

for i in 1 to 7: build an XGBClassifier to predict the probability that the observation has a Response value higher than i (for each of the seven iterations, the keyword-value-meantarget variables were calculated for that target variable)
for each observation, take the sum of these seven predicted probabilities as the overall prediction
this yields quite a bit better correlation with the target variable (and thus good raw material for calibration) than using an XGB regressor
Calibration:

the aim is to find the boundaries that maximize the kappa score
boundaries are initialized according to the original Response distribution of the training dataset
then in a step, for all boundaries, possible boundary values are examined in a small range around the current boundary value and the boundary is set to the value which gives the most improvement in kappa (independently of the other boundaries - this was surprising that it worked so well)
steps are repeated until none of the boundaries are changed during a step
it is a quite naive algorithm, but it turned out to be fairly robust and efficient
this was done on predictions generated by repeated crossvalidation using the XGBClassifier combo
Variable split calibration:

the difference here is that the crossvalidated preds are split into two subsets, based on some binary variable value (eg. a Medical_Keyword variable) of the observations
calibration then takes place for the two subsets separately (but with a kappa objective calculated over the entire set), in the manner described above
I didn't find an exact rule for picking a good splitting variable (strong correlation with Response seems to be necessary, but does not guarantee a good split), so I tried several (some of which were better than non-splitting calibration, others were worse)
for example, some good ones were: Medical_History_23, Medical_History_4, InsuredInfo6
also tried splitting into more than 2 subsets, without much success
Ensembling:

disregarding the combination of the 7 XGBClassifiers, the only ensembling I did was creating some combined solutions by taking the median predictions of a small number of other solutions
Evaluating calibrations:

K-fold crossvalidation, but with an important twist: each test fold was "cross-validated" again to imitate public/private test set split (the inner crossvalidation had a k of 3 to approximate the 30-70 leaderboard split)
this yielded a very interesting insight: given two calibrations with roughly equal average performance (over all folds), if calibration A does better on the public test set, calibration B is very likely to outperform A on the private set (this appears to be a quirk of the kappa metric)
accordingly, I picked the solutions which ranked #2 and #5 on the public leaderboard, since these both had very strong average performance in crossvalidation but slightly underperformed on the public leaderboard
Final results:

as it turned out, despite having the right idea about public/private error, I underestimated some solutions which had relatively weak average performance in crossvalidation but ended up doing extremely well on private
I did not select my best private submission for the final two (highest private score was 0.68002)
out of my 11 'high-tech' (that is, using all the modeling and calibration techniques listed above) submissions, 5 were good enough for 1st place on the private board, 4 would place 2nd, one would reach 6th, and the worst would yield 7th place (at least I can say that I had no intention of picking any of the latter two)
if my calculations are right, randomly selecting two out of the 11 would have resulted in 1st place with a probability of ~72.7 %
GÃ¡bor

#https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/4797/starter-code-in-python-with-scikit-learn-auc-885

Since it seems we have a lot of Coursera students with us (of which I'm also a fervent user), I wanted to share some simple starter code in Python to help those who are new at machine learning. You might also want to read Foxtrot's excellent post about beating the benchmark using Vowpal Wabbit. In contrast, this code uses Python with scikit-learn, and aims at giving a base on which to expand for those who want to be a little bit more hands-on or have more flexibility in the algorithm design.
It provides an example on how to design a simple algorithm, including performing some pre-processing, training a logistic regression classifier on the data, and assessing its performance through cross-validation. I also added some comments to point at where to go next. The script assumes you have train.csv and test.csv in a folder named data in the same location as the classifier.py file.
The strategy itself is essentially the same as Foxtrot's, ie. training a linear model on the original data with nothing else changed excepted for the one-hot encoding. In this case, the model used is a regularized logistic regression. This will net you an AUC score of .885 -- have fun!
Edit: forgot to remove an import in the original file. Use classifier_corrected.py instead.

==========
Wen: I definitely agree with you in that you learn the most by trying approaches that don't work, understanding why, then iterating. However, the main point of the starter code is to provide an example on how to design an algorithm from start to finish using scikit-learn, not just an out-of-the box solution (like if I told you to let Vowpal Wabbit do the whole work). The code itself is fairly generic and is designed to be modified; all you have to do is to change the preprocessing part or the line calling the LogisticRegression class if you want to try another method. Since I already provide a simple way to compute a cross-validation AUC score, you will then be able to directly see how your changes affect performance, then react accordingly.

Also, in this specific case the fact the solution gives a rather high AUC right off the bat is just due to the characteristics of the dataset -- the method itself is fairly straightforward. I also don't think progress is necessarily incremental, since sometimes I'll try a technique that actually gives me worse performance than what I had. Sometimes it's worse but better in some cases, and realizing that can help you in tern improve your previous method. Having a good score already doesn't prevent you from experimenting. 

davyzhu: Sure. In your case, I would advise to not spend too much effort on feature selection right now (although you are definitely welcome to try). The reason is that at this stage, your features are essentially based on the 8 columns you were provided at the beginning and each correspond to a specific manager, position, department and so on, so they pretty much all do provide some useful information (if you delete a feature corresponding to manager 23421, for example, you will most likely be worse off when trying to predict the outcome of a resource request made by someone with the same manager).
Sklearn provides a module called feature_selection, which implements quite a few classes and methods you can use for feature selection. Be careful with recursive feature selection algorithms though, since they're not appropriate for very high dimensional data. 
For hyperparameter optimization, the simplest way to do it is just by modifying the parameters by hand, then looking at how this affects the cross validation score. This can work if you only have one parameter to optimize, but when you have several the usual way to do it is by trying a lot of different possibilities by grid search. Scikit-learn also provides classes for that: http://scikit-learn.org/0.13/modules/grid_search.html (in particular, look at GridSearchCV)
Also, the reason I put this comment inside the cross-validation loop is that feature selection and hyperparameter optimization basically fit the model itself to the data, so you'll end up getting a CV score that is way too optimistic otherwise. As a rule of thumb, any method that use some information from the labels should be done inside a CV loop -- in the case of the code I provided, you want to fit your GridSearchCV object to (X_train, y_train) and not directly (X, y).

==========
